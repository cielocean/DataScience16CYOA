{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration\n",
    "Date Created: 14 February 2016\n",
    "\n",
    "This model iteration is used to make crime category predictions for test data for San Francisco Crime Classification kaggle competition\n",
    "https://www.kaggle.com/c/sf-crime\n",
    "\n",
    "as of 14.02.16\n",
    "Rank: /\n",
    "Score: %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Importing Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing train dataset\n",
    "z_train = zipfile.ZipFile('train.csv.zip')\n",
    "train = pd.read_csv(z_train.open('train.csv'), parse_dates=['Dates'], index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing test dataset\n",
    "z_test = zipfile.ZipFile('test.csv.zip')\n",
    "test = pd.read_csv(z_test.open('test.csv'), parse_dates=['Dates'], index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying and Trimming Data\n",
    "\n",
    "Here, we analyze data and modify it accordingly. As we see the data columns for the training and testing data, we see that the resolution column is not really needed. Moreover, some data types such as PdDistrict and Address seem to have some overlap, so we may pick to use one of them, or some altered version of each. Also, we dropped the Descript column from the train data will be dropped as there are great number of unique values, and are not present in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Data columns (total 9 columns):\n",
      "Dates         878049 non-null datetime64[ns]\n",
      "Category      878049 non-null object\n",
      "Descript      878049 non-null object\n",
      "DayOfWeek     878049 non-null object\n",
      "PdDistrict    878049 non-null object\n",
      "Resolution    878049 non-null object\n",
      "Address       878049 non-null object\n",
      "X             878049 non-null float64\n",
      "Y             878049 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), object(6)\n",
      "memory usage: 67.0+ MB\n",
      "None\n",
      "----------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 7 columns):\n",
      "Id            884262 non-null int64\n",
      "Dates         884262 non-null datetime64[ns]\n",
      "DayOfWeek     884262 non-null object\n",
      "PdDistrict    884262 non-null object\n",
      "Address       884262 non-null object\n",
      "X             884262 non-null float64\n",
      "Y             884262 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(3)\n",
      "memory usage: 54.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print train.info()\n",
    "print \"----------------------------------\"\n",
    "print test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "The information in some of the columns in the data are extracted & seperated into different columns for better evaluation. \n",
    "\n",
    "The time_trim function converts total Date column into sub parts for convenience.\n",
    "\n",
    "The make_binary_fields function, allows us to create dummy variables with data from pre-existing columns. Dummy variables work extremely well with Random Forrest Regression, although the number of columns in the data set are increased. This will probably be used for randomforest or gradient boosting method.\n",
    "\n",
    "The make_seasons function converts month data into seasons which we can include in our approximation. However, seasons overlap with date greatly, so we may not fully utilize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modifying Functions\n",
    "\n",
    "From a source we used from the kaggle scripts, we create a streamlined function that adds new time categories to the data set based off of the 'Dates' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_trim(df):\n",
    "    df['Day'] = df['Dates'].dt.day\n",
    "    df['Month'] = df['Dates'].dt.month\n",
    "    df['Year'] = df['Dates'].dt.year\n",
    "    df['Hour'] = df['Dates'].dt.hour\n",
    "    df['Minute'] = df['Dates'].dt.minute\n",
    "    df['WeekOfYear'] = df['Dates'].dt.weekofyear\n",
    "    return\n",
    "\n",
    "def make_binary_fields(df, field):\n",
    "    \"\"\"\n",
    "    creates new field with field name as the name of data \n",
    "    if the original data match the new field name, the data will be 1\n",
    "    if the original data does not match the new field name, the data will be 0\n",
    "\n",
    "    \n",
    "    ex \n",
    "    make_binary_field(df, 'DayOfWeek')\n",
    "    will create new fields\n",
    "    Monday, Tuesday, Wednesday, Thursday, Friday, Saturday and Sunday\n",
    "    where\n",
    "    df['Monday'] will have value 1 for all Mondays and 0 for the rest\n",
    "    \"\"\"\n",
    "    for new_field in df[field].unique():\n",
    "        df[new_field] = df[field]\n",
    "        df.loc[df[new_field] != new_field, new_field] = 0\n",
    "        df.loc[df[new_field] == new_field, new_field] = 1\n",
    "    return\n",
    "\n",
    "def make_season(df):\n",
    "    \"\"\"\n",
    "    Make new field name Season\n",
    "    and binary fields for each season\n",
    "    Has to happen after making 'Month' field\n",
    "    spring: month 2, 3, 4\n",
    "    summer: month 5, 6, 7\n",
    "    autumn: month 8, 9, 10\n",
    "    winter: month 11, 12, 1\n",
    "    \"\"\"\n",
    "    df['Season'] = df['Month']\n",
    "    df.loc[(df['Season'] > 10) | (df['Season'] == 1), 'Season'] = 'Winter'\n",
    "    df.loc[(df['Season'] > 1) & (df['Season'] <= 4), 'Season'] = 'Spring'\n",
    "    df.loc[(df['Season'] > 4) & (df['Season'] <= 7), 'Season'] = 'Summer'\n",
    "    df.loc[(df['Season'] > 7) & (df['Season'] <= 10), 'Season'] = 'Autumn'\n",
    "    seasons = make_binary_fields(df, 'Season')\n",
    "    return seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting data\n",
    "\n",
    "Here we alter the columns we have in our data set to suit our regression procedure. Because many of the sklearn regression functions cannot handle string data, we must convert them to either dummie variables or re-encode them with numbers. With the make_binary_fields and time_trim function, we do such for DayOfWeek, PdDistrict, and with LabelEncoder() we do such for the Categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_trim(train)\n",
    "seasons = make_season(train)\n",
    "#dow = make_binary_fields(train, 'DayOfWeek')\n",
    "#pdd = make_binary_fields(train, 'PdDistrict')\n",
    "#categories = make_binary_fields(train, 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_pdd = LabelEncoder()\n",
    "train['PdDistrict'] = enc_pdd.fit_transform(train['PdDistrict'])\n",
    "test['PdDistrict'] = enc_pdd.fit_transform(test['PdDistrict'])\n",
    "enc_seas = LabelEncoder()\n",
    "train['Season'] = enc_seas.fit_transform(train['Season'])\n",
    "enc_dow = LabelEncoder()\n",
    "train['DayOfWeek'] = enc_dow.fit_transform(train['DayOfWeek'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "enc.fit(train['Category'])\n",
    "train['CategoryEncoded'] = enc.transform(train['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting predictors\n",
    "\n",
    "The predictors are the data columns we will want to use for our regression process. We added the time components, and appended the PdDistric, Seasons, and day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add time categories\n",
    "predictors = ['Day','Month','Year','Hour','Minute','WeekOfYear','PdDistrict','Season',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle SF Crime Classification Scoring System\n",
    "\n",
    "We noticed on the leader board of the SF Crime Kaggle the scoring system was logloss. Therefore, based upon someone else's algorithm on calculating log loss we developed our own. Here we initially get rid of extreme values (by designating an epsilon) and then create a log loss return that is our score.\n",
    "\n",
    "The log loss sytem works differently from the regular percentage scoring system. When computing the output for the test regression, each category (the point of interest) will be assigned a probability based upon the regression. With that, the solution is compared to the output of the test regression, and the total loss (or inaccuracy of each probability) is computed and summed. Overall, we want a lower log loss value for a more successful regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(y,p):\n",
    "    \"\"\"\n",
    "    information derived from following sources\n",
    "    https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "    https://www.kaggle.com/c/sf-crime/details/evaluation\n",
    "    \"\"\"\n",
    "    eps = 1e-15\n",
    "    p = p/p.sum(axis=1)[:,np.newaxis]\n",
    "    p = np.maximum(eps,p)\n",
    "    p = np.minimum(1-eps,p)\n",
    "    \n",
    "\n",
    "    # Calculate logloss\n",
    "    ll = 0\n",
    "    for i in range(len(p)):\n",
    "        ll += np.log(p[i, y.iloc[i]])\n",
    "    ll /= float(-len(p))\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating train data\n",
    "\n",
    "Here, we create folds in our data to split our training data into a train set and a test set. With this, we can test within our train data to see how accurate our model is before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create x and y from train data, x will be train, y will be target\n",
    "x = train[predictors]\n",
    "y = train['CategoryEncoded']\n",
    "xtr, xtest, ytr, ytest = cross_validation.train_test_split(x, y, test_size = 0.5, stratify = np.array(y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Our first regression attempt is the logistic regression. We are familiar with this process from our warm up project and it is a good starting point. However, I believe that since the point of interest (category) has 38 parts, the logistic regression may not be apt, as during the Warmup Project the output was either alive or dead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6278127069582111"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = LogisticRegression()\n",
    "alg.fit(xtr, ytr)\n",
    "prediction = alg.predict_proba(xtest)\n",
    "logloss(ytest,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "The decision tree uses a lot of conditional statements. Because we moved a lot of our categories to dummie variables, (with either true or false conditions), we believe that the decision tree model will be very suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.55875519637353"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = sk.tree.DecisionTreeClassifier(max_depth=4, min_samples_leaf=4)\n",
    "alg.fit(xtr, ytr)\n",
    "prediction = alg.predict_proba(xtest)\n",
    "logloss(ytest,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient boosting is based upon the decision tree. The gradient boosting method implements many methods, including the decision tree. This is a venture from our decision tree, and is related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6825610696768618"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=10, max_depth=3)\n",
    "alg.fit(xtr, ytr)\n",
    "prediction = alg.predict_proba(xtest)\n",
    "logloss(ytest,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
