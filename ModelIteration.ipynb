{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Iteration\n",
    "Date Created: 14 February 2016\n",
    "\n",
    "This model iteration is used to make crime category predictions for test data for San Francisco Crime Classification kaggle competition\n",
    "https://www.kaggle.com/c/sf-crime\n",
    "\n",
    "as of 14.02.16\n",
    "Rank: /\n",
    "Score: %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Importing Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing train dataset\n",
    "z_train = zipfile.ZipFile('train.csv.zip')\n",
    "train = pd.read_csv(z_train.open('train.csv'), parse_dates=['Dates'], index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing test dataset\n",
    "z_test = zipfile.ZipFile('test.csv.zip')\n",
    "test = pd.read_csv(z_test.open('test.csv'), parse_dates=['Dates'], index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying and Trimming Data\n",
    "\n",
    "Here, we analyze data and modify it accordingly. As we see the data columns for the training and testing data, we see that the resolution column is not really needed. Moreover, some data types such as PdDistrict and Address seem to have some overlap, so we may pick to use one of them, or some altered version of each. Also, we dropped the Descript column from the train data will be dropped as there are great number of unique values, and are not present in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 878049 entries, 0 to 878048\n",
      "Data columns (total 9 columns):\n",
      "Dates         878049 non-null datetime64[ns]\n",
      "Category      878049 non-null object\n",
      "Descript      878049 non-null object\n",
      "DayOfWeek     878049 non-null object\n",
      "PdDistrict    878049 non-null object\n",
      "Resolution    878049 non-null object\n",
      "Address       878049 non-null object\n",
      "X             878049 non-null float64\n",
      "Y             878049 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), object(6)\n",
      "memory usage: 67.0+ MB\n",
      "None\n",
      "----------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 884262 entries, 0 to 884261\n",
      "Data columns (total 7 columns):\n",
      "Id            884262 non-null int64\n",
      "Dates         884262 non-null datetime64[ns]\n",
      "DayOfWeek     884262 non-null object\n",
      "PdDistrict    884262 non-null object\n",
      "Address       884262 non-null object\n",
      "X             884262 non-null float64\n",
      "Y             884262 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(3)\n",
      "memory usage: 54.0+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print train.info()\n",
    "print \"----------------------------------\"\n",
    "print test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "The information in some of the columns in the data are extracted & seperated into different columns for better evaluation. \n",
    "\n",
    "The extract_date function alters the Dates column in the data set to be used more conveniently. Previously, one column held all of year, month, day, and specific time, but now we divide it up. By doing this we can see trends in crime within different days, different years, and a lot of flexibility becomes available.\n",
    "\n",
    "The extract_time function divides the Time column into Hour, Minute and Second. We will mainly be using the time column\n",
    "\n",
    "The make_binary_features function, allows us to create dummy variables with data from pre-existing columns. Dummy variables work extremely well with Random Forrest Regression, although the number of columns in the data set are increased. This will probably be used for randomforest or gradient boosting method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Functions\n",
    "\n",
    "From a source we used from the kaggle scripts, we create a streamlined function that adds new time categories to the data set based off of the 'Dates' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_trim(df):\n",
    "    df['Day'] = df['Dates'].dt.day\n",
    "    df['Month'] = df['Dates'].dt.month\n",
    "    df['Year'] = df['Dates'].dt.year\n",
    "    df['Hour'] = df['Dates'].dt.hour\n",
    "    df['Minute'] = df['Dates'].dt.minute\n",
    "    df['WeekOfYear'] = df['Dates'].dt.weekofyear\n",
    "    return\n",
    "\n",
    "def make_binary_fields(df, field):\n",
    "    \"\"\"\n",
    "    creates new field with field name as the name of data \n",
    "    if the original data match the new field name, the data will be 1\n",
    "    if the original data does not match the new field name, the data will be 0\n",
    "\n",
    "    \n",
    "    ex \n",
    "    make_binary_field(df, 'DayOfWeek')\n",
    "    will create new fields\n",
    "    Monday, Tuesday, Wednesday, Thursday, Friday, Saturday and Sunday\n",
    "    where\n",
    "    df['Monday'] will have value 1 for all Mondays and 0 for the rest\n",
    "    \"\"\"\n",
    "    for new_field in df[field].unique():\n",
    "        df[new_field] = df[field]\n",
    "        df.loc[df[new_field] != new_field, new_field] = 0\n",
    "        df.loc[df[new_field] == new_field, new_field] = 1\n",
    "    return\n",
    "\n",
    "def make_season(df):\n",
    "    \"\"\"\n",
    "    Make new field name Season\n",
    "    and binary fields for each season\n",
    "    Has to happen after making 'Month' field\n",
    "    spring: month 2, 3, 4\n",
    "    summer: month 5, 6, 7\n",
    "    autumn: month 8, 9, 10\n",
    "    winter: month 11, 12, 1\n",
    "    \"\"\"\n",
    "    df['Season'] = df['Month']\n",
    "    df.loc[(df['Season'] > 10) | (df['Season'] == 1), 'Season'] = 'Winter'\n",
    "    df.loc[(df['Season'] > 1) & (df['Season'] <= 4), 'Season'] = 'Spring'\n",
    "    df.loc[(df['Season'] > 4) & (df['Season'] <= 7), 'Season'] = 'Summer'\n",
    "    df.loc[(df['Season'] > 7) & (df['Season'] <= 10), 'Season'] = 'Autumn'\n",
    "    seasons = make_binary_fields(df, 'Season')\n",
    "    return seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_trim(train)\n",
    "seasons = make_season(train)\n",
    "dow = make_binary_fields(train, 'DayOfWeek')\n",
    "pdd = make_binary_fields(train, 'PdDistrict')\n",
    "categories = make_binary_fields(train, 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "enc.fit(train['Category'])\n",
    "train['CategoryEncoded'] = enc.transform(train['Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = ['Day','Month','Year','Hour','Minute','WeekOfYear']\n",
    "predictors.extend(pdd)\n",
    "predictors.extend(seasons)\n",
    "predictors.extend(dow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle SF Crime Classification Scoring System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss(y,p):\n",
    "    \"\"\"\n",
    "    information derived from following sources\n",
    "    https://www.kaggle.com/wiki/LogarithmicLoss\n",
    "    https://www.kaggle.com/c/sf-crime/details/evaluation\n",
    "    \"\"\"\n",
    "    eps = 1e-15\n",
    "    p = p/p.sum(axis=1)[:,np.newaxis]\n",
    "    p = np.maximum(eps,p)\n",
    "    p = np.minimum(1-eps,p)\n",
    "    \n",
    "\n",
    "    # Calculate logloss\n",
    "    ll = 0\n",
    "    for i in range(len(p)):\n",
    "        ll += np.log(p[i, y.iloc[i]])\n",
    "    ll /= float(-len(p))\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train[predictors]\n",
    "y = train['CategoryEncoded']\n",
    "xtr, xtest, ytr, ytest = cross_validation.train_test_split(x, y, test_size = 0.5, stratify = np.array(y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = LogisticRegression()\n",
    "alg.fit(xtr, ytr)\n",
    "prediction = alg.predict_proba(xtest)\n",
    "logloss(ytest,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = sk.tree.DecisionTreeClassifier(max_depth=4, min_samples_leaf=4)\n",
    "alg.fit(xtr, ytr)\n",
    "prediction = alg.predict_proba(xtest)\n",
    "logloss(ytest,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alg = GradientBoostingClassifier(random_state=1, n_estimators=10, max_depth=3)\n",
    "alg.fit(xtr, ytr)\n",
    "prediction = alg.predict_proba(xtest)\n",
    "logloss(ytest,prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
